[
    "Google announces Firebase Genkit with Ollama support\nMay 20, 2024\nAt Google IO 2024, Google announced Ollama support in Firebase Genkit, a new\nopen-source framework for developers to build, deploy and monitor production-\nready AI-powered apps.\nLlama 3 is not very censored\nApril 19, 2024\nCompared to Llama 2, Llama 3 feels much less censored. Meta has substantially\nlowered false refusal rates. Llama 3 will refuse less than 1\/3 of the prompts\npreviously refused by Llama 2.\nLlama 3\nApril 18, 2024\nLlama 3 is now available to run on Ollama. This model is the next generation of\nMeta's state-of-the-ar",
    "ama 3 is now available to run on Ollama. This model is the next generation of\nMeta's state-of-the-art large language model, and is the most capable openly\navailable LLM to date.\nEmbedding models\nApril 8, 2024\nBlogDiscordGitHub\tModelsSign in Download ",
    "Embedding models are available in Ollama, making it easy to generate vector\nembeddings for use in search and retrieval augmented generation (RAG)\napplications.\nOllama now supports AMD graphics cards\nMarch 14, 2024\nOllama now supports AMD graphics cards in preview on Windows and Linux. All the\nfeatures of Ollama can now be accelerated by AMD graphics cards on Ollama for\nLinux and Windows.\nWindows preview\nFebruary 15, 2024\nOllama is now available on Windows in preview, making it possible to pull, run and\ncreate large language models in a new native Windows experience. Ollama on\nWindows includes ",
    "run and\ncreate large language models in a new native Windows experience. Ollama on\nWindows includes built-in GPU acceleration, access to the full model library, and\nserves the Ollama API including OpenAI compatibility.\nOpenAI compatibility\nFebruary 8, 2024\nOllama now has initial compatibility with the OpenAI Chat Completions API, making\nit possible to use existing tooling built for OpenAI with local models via Ollama.\nVision models\nFebruary 2, 2024 ",
    "New vision models are now available: LLaVA 1.6, in 7B, 13B and 34B parameter\nsizes. These models support higher resolution images, improved text recognition\nand logical reasoning.\nPython & JavaScript Libraries\nJanuary 23, 2024\nThe initial versions of the Ollama Python and JavaScript libraries are now available,\nmaking it easy to integrate your Python or JavaScript, or Typescript app with Ollama\nin a few lines of code. Both libraries include all the features of the Ollama REST API,\nare familiar in design, and compatible with new and previous versions of Ollama.\nBuilding LLM-Powered Web Apps wit",
    "n design, and compatible with new and previous versions of Ollama.\nBuilding LLM-Powered Web Apps with Client-Side Technology\nOctober 13, 2023\nRecreate one of the most popular LangChain use-cases with open source, locally\nrunning software - a chain that performs Retrieval-Augmented Generation, or RAG\nfor short, and allows you to \u201cchat with your documents\u201d\nOllama is now available as an official Docker image\nOctober 5, 2023\nOllama can now run with Docker Desktop on the Mac, and run inside Docker\ncontainers with GPU acceleration on Linux.\nLeveraging LLMs in your Obsidian Notes\nSeptember 21, 20",
    "r\ncontainers with GPU acceleration on Linux.\nLeveraging LLMs in your Obsidian Notes\nSeptember 21, 2023\nThis post walks through how you could incorporate a local LLM using Ollama in\nObsidian, or potentially any note taking tool. ",
    "How to prompt Code Llama\nSeptember 9, 2023\nThis guide walks through the different ways to structure prompts for Code Llama\nand its different variations and features including instructions, code completion and\nfill-in-the-middle (FIM).\nRun Code Llama locally\nAugust 24, 2023\nMeta's Code Llama is now available on Ollama to try.\nRun WizardMath model for math problems\nAugust 14, 2023\nRun WizardMath model for math problems\nRun Llama 2 uncensored locally\nAugust 1, 2023\nThis post will give some example comparisons running Llama 2 uncensored model\nversus its censored model.\n\u00a9 2024 Ollama\tBlog Docs Git",
    "comparisons running Llama 2 uncensored model\nversus its censored model.\n\u00a9 2024 Ollama\tBlog Docs GitHub Discord X (Twitter)Meetups ",
    "leased by Mistral AI, updated to version 0.2.\n7B\nFilter by name...\nBlogDiscordGitHub\tModelsSign in Download ",
    "\u00a0Tags Updated\u00a03 months ago\ncodegemma\nCodeGemma is a collection of powerful, lightweight models\nthat can perform a variety of coding tasks like fill-in-the-\nmiddle code completion, code generation, natural language\nunderstanding, mathematical reasoning, and instruction\nfollowing.\n2B 7B\n80.3K\u00a0Pulls 85\u00a0Tags Updated\u00a02 weeks ago\ncommand-r ",
    "eral-purpose model ranging from 3 billion parameters to\n70 billion, suitable for entry-level hardware.\n3B 7B 13B 65B\n98.8K\u00a0Pulls 119\u00a0Tags Updated\u00a06 months ago ",
    "er2 is the next generation of transparently trained\nopen code LLMs that comes in three sizes: 3B, 7B and 15B\nparameters.\n3B 7B 16B\n62.2K\u00a0Pulls 67\u00a0Tags Updated\u00a02 weeks ago\nwizard-vicuna-uncensored\nWizard Vicuna Uncensored is a 7B, 13B, and 30B parameter\nmodel based on Llama 2 uncensored by Eric Hartford.\n7B 13B 30B\n62K\u00a0Pulls 49\u00a0Tags Updated\u00a06 months ago ",
    "70B sizes by Eric\nHartford based on Llama 3 that has a variety of instruction,\nconversational, and coding skills.\n8B 70B\n50.2K\u00a0Pulls 54\u00a0Tags Updated\u00a010 days ago\nyi\nYi 1.5 is a high-performing, bilingual language model.\n6B 9B 34B\n49.6K\u00a0Pulls 174\u00a0Tags Updated\u00a08 days ago ",
    " months ago\nmxbai-embed-large\nState-of-the-art large embedding model from mixedbread.ai\n334M\n44.2K\u00a0Pulls 4\u00a0Tags Updated\u00a02 weeks ago\nwizardcoder\nState-of-the-art code generation model\n7B 13B 33B 34B\n43.3K\u00a0Pulls 67\u00a0Tags Updated\u00a04 months ago ",
    "based model with support for a 16K\ncontext window.\n13B\n14.7K\u00a0Pulls 18\u00a0Tags Updated\u00a04 months ago\nmeditron ",
    "Models\nFeatured\nllama3\nMeta Llama 3: The most capable openly available LLM to date\n8B 70B\n1.4M\u00a0Pulls 68\u00a0Tags Updated\u00a014 hours ago\nphi3\nPhi-3 Mini is a 3.8B parameters, lightweight, state-of-the-art\nopen model by Microsoft.\n4B\n207.9K\u00a0Pulls 6\u00a0Tags Updated\u00a03 weeks ago\nwizardlm2\nState of the art large language model from Microsoft AI with\nimproved performance on complex chat, multilingual,\nreasoning and agent use cases.\n7B 141B\n53.8K\u00a0Pulls 22\u00a0Tags Updated\u00a04 weeks ago\nmistral\nThe 7B model released by Mistral AI, updated to version 0.2.\n7B\nFilter by name...\nBlogDiscordGitHub\tModelsSign in D",
    "789.9K\u00a0Pulls 68\u00a0Tags Updated\u00a08 weeks ago\ngemma\nGemma is a family of lightweight, state-of-the-art open\nmodels built by Google DeepMind. Updated to version 1.1\n2B 7B\n1.6M\u00a0Pulls 102\u00a0Tags Updated\u00a06 weeks ago\nmixtral\nA set of Mixture of Experts (MoE) model with open weights by\nMistral AI in 8x7b and 8x22b parameter sizes.\n47B 141B\n246.1K\u00a0Pulls 69\u00a0Tags Updated\u00a04 days ago\nllama2\nLlama 2 is a collection of foundation language models ranging\nfrom 7B to 70B parameters.\n7B 13B 70B\n1.6M\u00a0Pulls 102\u00a0Tags Updated\u00a03 months ago\ncodegemma\nCodeGemma is a collection of powerful, lightweight models\ntha",
    "Command R is a Large Language Model optimized for\nconversational interaction and long context tasks.\n35B\n37.4K\u00a0Pulls 17\u00a0Tags Updated\u00a07 weeks ago\ncommand-r-plus\nCommand R+ is a powerful, scalable large language model\npurpose-built to excel at real-world enterprise use cases.\n104B\n30K\u00a0Pulls 6\u00a0Tags Updated\u00a04 weeks ago\nllava\n LLaVA is a novel end-to-end trained large multimodal\nmodel that combines a vision encoder and Vicuna for\ngeneral-purpose visual and language understanding.\nUpdated to version 1.6.\n7B 13B 34B\n228.5K\u00a0Pulls 98\u00a0Tags Updated\u00a03 months ago\ndbrx\nDBRX is an open, genera",
    "to version 1.6.\n7B 13B 34B\n228.5K\u00a0Pulls 98\u00a0Tags Updated\u00a03 months ago\ndbrx\nDBRX is an open, general-purpose LLM created by\nDatabricks.\n132B\n6,655\u00a0Pulls 7\u00a0Tags Updated\u00a04 weeks ago\ncodellama\nA large language model that can use text prompts to generate\nand discuss code.\n7B 13B 34B 69B\n461.2K\u00a0Pulls 199\u00a0Tags Updated\u00a010 days ago ",
    "qwen\nQwen 1.5 is a series of large language models by Alibaba\nCloud spanning from 0.5B to 110B parameters\n4B 8B 14B 32B 110B 0.5B 2B 72B\n372.5K\u00a0Pulls 379\u00a0Tags Updated\u00a03 weeks ago\ndolphin-mixtral\nUncensored, 8x7b and 8x22b fine-tuned models based on\nthe Mixtral mixture of experts models that excels at coding\ntasks. Created by Eric Hartford.\n47B 141B\n243.1K\u00a0Pulls 87\u00a0Tags Updated\u00a02 weeks ago\nllama2-uncensored\nUncensored Llama 2 model by George Sung and Jarrad\nHope.\n7B 65B\n192.3K\u00a0Pulls 34\u00a0Tags Updated\u00a06 months ago\ndeepseek-coder\nDeepSeek Coder is a capable coding model trained on two\ntril",
    "gs Updated\u00a06 months ago\ndeepseek-coder\nDeepSeek Coder is a capable coding model trained on two\ntrillion code and natural language tokens.\n1B 7B 33B\n144K\u00a0Pulls 102\u00a0Tags Updated\u00a04 months ago\nmistral-openorca\nMistral OpenOrca is a 7 billion parameter model, fine-tuned\non top of the Mistral 7B model using the OpenOrca dataset.\n7B\n124.6K\u00a0Pulls 17\u00a0Tags Updated\u00a07 months ago ",
    "nomic-embed-text\nA high-performing open embedding model with a large token\ncontext window.\n137M\n116.2K\u00a0Pulls 3\u00a0Tags Updated\u00a02 months ago\ndolphin-mistral\nThe uncensored Dolphin model based on Mistral that excels\nat coding tasks. Updated to version 2.8.\n7B\n106.1K\u00a0Pulls 120\u00a0Tags Updated\u00a07 weeks ago\nphi\nPhi-2: a 2.7B language model by Microsoft Research that\ndemonstrates outstanding reasoning and language\nunderstanding capabilities.\n3B\n102K\u00a0Pulls 18\u00a0Tags Updated\u00a03 months ago\norca-mini\nA general-purpose model ranging from 3 billion parameters to\n70 billion, suitable for entry-level hardwar",
    "nous-hermes2\nThe powerful family of models by Nous Research that excels\nat scientific discussion and coding tasks.\n11B 34B\n80.7K\u00a0Pulls 33\u00a0Tags Updated\u00a04 months ago\nzephyr\nZephyr is a series of fine-tuned versions of the Mistral and\nMixtral models that are trained to act as helpful assistants.\n7B 141B\n76.1K\u00a0Pulls 40\u00a0Tags Updated\u00a05 weeks ago\nllama2-chinese\nLlama 2 based model fine tuned to improve Chinese dialogue\nability.\n7B 13B\n66.6K\u00a0Pulls 35\u00a0Tags Updated\u00a06 months ago\nstarcoder2\nStarCoder2 is the next generation of transparently trained\nopen code LLMs that comes in three sizes: 3B, 7B",
    "vicuna\nGeneral use chat model based on Llama and Llama 2 with 2K\nto 16K context sizes.\n7B 13B 30B\n60.5K\u00a0Pulls 111\u00a0Tags Updated\u00a06 months ago\ntinyllama\nThe TinyLlama project is an open endeavor to train a compact\n1.1B Llama model on 3 trillion tokens.\n1B\n53.4K\u00a0Pulls 36\u00a0Tags Updated\u00a04 months ago\nopenhermes\nOpenHermes 2.5 is a 7B model fine-tuned by Teknium on\nMistral with fully open datasets.\n7B\n51.4K\u00a0Pulls 35\u00a0Tags Updated\u00a04 months ago\ndolphin-llama3\nDolphin 2.9 is a new model with 8B and 70B sizes by Eric\nHartford based on Llama 3 that has a variety of instruction,\nconversational, and c",
    "starcoder\nStarCoder is a code generation model trained on 80+\nprogramming languages.\n1B 3B 7B 15B\n49.2K\u00a0Pulls 100\u00a0Tags Updated\u00a07 months ago\nopenchat\nA family of open-source models trained on a wide variety of\ndata, surpassing ChatGPT on various benchmarks. Updated\nto version 3.5-0106.\n7B\n48.4K\u00a0Pulls 50\u00a0Tags Updated\u00a04 months ago\ntinydolphin\nAn experimental 1.1B parameter model trained on the new\nDolphin 2.8 dataset by Eric Hartford and based on TinyLlama.\n1B\n44.5K\u00a0Pulls 18\u00a0Tags Updated\u00a03 months ago\nmxbai-embed-large\nState-of-the-art large embedding model from mixedbread.ai\n334M\n44.2K\u00a0",
    "stable-code\nStable Code 3B is a coding model with instruct and code\ncompletion variants on par with models such as Code Llama\n7B that are 2.5x larger.\n3B\n43K\u00a0Pulls 36\u00a0Tags Updated\u00a08 weeks ago\nneural-chat\nA fine-tuned model based on Mistral with good coverage of\ndomain and language.\n7B\n35.8K\u00a0Pulls 50\u00a0Tags Updated\u00a07 weeks ago\nphind-codellama\nCode generation model based on Code Llama.\n34B\n32.5K\u00a0Pulls 49\u00a0Tags Updated\u00a04 months ago\nwizard-math\nModel focused on math and logic problems\n7B 13B 65B\n32.1K\u00a0Pulls 64\u00a0Tags Updated\u00a05 months ago\nstarling-lm\nStarling is a large language model traine",
    "5B\n32.1K\u00a0Pulls 64\u00a0Tags Updated\u00a05 months ago\nstarling-lm\nStarling is a large language model trained by reinforcement\nlearning from AI feedback focused on improving chatbot\nhelpfulness.\n7B\n28.5K\u00a0Pulls 36\u00a0Tags Updated\u00a06 weeks ago ",
    "falcon\nA large language model built by the Technology Innovation\nInstitute (TII) for use in summarization, text generation, and\nchat bots.\n7B 40B 180B\n27.1K\u00a0Pulls 38\u00a0Tags Updated\u00a06 months ago\ndolphincoder\nA 7B and 15B uncensored variant of the Dolphin model family\nthat excels at coding, based on StarCoder2.\n7B 16B\n26.8K\u00a0Pulls 35\u00a0Tags Updated\u00a06 weeks ago\nnous-hermes\nGeneral use models based on Llama and Llama 2 from Nous\nResearch.\n7B 13B 65B\n26.3K\u00a0Pulls 63\u00a0Tags Updated\u00a06 months ago\norca2\nOrca 2 is built by Microsoft research, and are a fine-tuned\nversion of Meta's Llama 2 models. The m",
    "\nOrca 2 is built by Microsoft research, and are a fine-tuned\nversion of Meta's Llama 2 models. The model is designed to\nexcel particularly in reasoning.\n7B 13B\n25.7K\u00a0Pulls 33\u00a0Tags Updated\u00a06 months ago\nstablelm2\nStable LM 2 is a state-of-the-art 1.6B and 12B parameter\nlanguage model trained on multilingual data in English,\nSpanish, German, Italian, French, Portuguese, and Dutch.\n2B 12B ",
    "25.1K\u00a0Pulls 84\u00a0Tags Updated\u00a02 weeks ago\nsqlcoder\nSQLCoder is a code completion model fined-tuned on\nStarCoder for SQL generation tasks\n7B 15B 69B\n24.6K\u00a0Pulls 48\u00a0Tags Updated\u00a03 months ago\ndolphin-phi\n2.7B uncensored Dolphin model by Eric Hartford, based on\nthe Phi language model by Microsoft Research.\n3B\n23.6K\u00a0Pulls 15\u00a0Tags Updated\u00a04 months ago\nsolar\nA compact, yet powerful 10.7B large language model\ndesigned for single-turn conversation.\n11B\n22K\u00a0Pulls 32\u00a0Tags Updated\u00a05 months ago\nyarn-llama2\nAn extension of Llama 2 that supports a context of up to 128k\ntokens.\n7B 13B\n21.1K\u00a0Pulls 6",
    "n-llama2\nAn extension of Llama 2 that supports a context of up to 128k\ntokens.\n7B 13B\n21.1K\u00a0Pulls 67\u00a0Tags Updated\u00a06 months ago\ndeepseek-llm\nAn advanced language model crafted with 2 trillion bilingual\ntokens.\n7B 67B ",
    "20.9K\u00a0Pulls 64\u00a0Tags Updated\u00a05 months ago\ncodeqwen\nCodeQwen1.5 is a large language model pretrained on a large\namount of code data.\n7B\n20.4K\u00a0Pulls 21\u00a0Tags Updated\u00a04 weeks ago\nllama3-gradient\nThis model extends LLama-3 8B's context length from 8k to\nover 1m tokens.\n8B 70B\n19.2K\u00a0Pulls 35\u00a0Tags Updated\u00a02 weeks ago\nbakllava\nBakLLaVA is a multimodal model consisting of the Mistral 7B\nbase model augmented with the LLaVA architecture.\n7B\n19.2K\u00a0Pulls 17\u00a0Tags Updated\u00a05 months ago\nall-minilm\nEmbedding models on very large sentence level datasets.\n23M 33M\n19.1K\u00a0Pulls 10\u00a0Tags Updated\u00a02 weeks ",
    "edding models on very large sentence level datasets.\n23M 33M\n19.1K\u00a0Pulls 10\u00a0Tags Updated\u00a02 weeks ago ",
    "samantha-mistral\nA companion assistant trained in philosophy, psychology, and\npersonal relationships. Based on Mistral.\n7B\n18.9K\u00a0Pulls 49\u00a0Tags Updated\u00a07 months ago\nmedllama2\nFine-tuned Llama 2 model to answer medical questions based\non an open source medical dataset.\n7B\n18.2K\u00a0Pulls 17\u00a0Tags Updated\u00a06 months ago\nxwinlm\nConversational model based on Llama 2 that performs\ncompetitively on various benchmarks.\n7B 13B 65B\n18.2K\u00a0Pulls 80\u00a0Tags Updated\u00a06 months ago\nwizardlm-uncensored\nUncensored version of Wizard LM model\n13B\n17.6K\u00a0Pulls 18\u00a0Tags Updated\u00a06 months ago\nstable-beluga\nLlama 2 bas",
    "version of Wizard LM model\n13B\n17.6K\u00a0Pulls 18\u00a0Tags Updated\u00a06 months ago\nstable-beluga\nLlama 2 based model fine tuned on an Orca-style dataset.\nOriginally called Free Willy.\n7B 13B 65B\n17.4K\u00a0Pulls 49\u00a0Tags Updated\u00a06 months ago ",
    "nous-hermes2-mixtral\nThe Nous Hermes 2 model from Nous Research, now trained\nover Mixtral.\n47B\n17.3K\u00a0Pulls 18\u00a0Tags Updated\u00a04 months ago\nwizardlm\nGeneral use model based on Llama 2.\n7B 13B 30B 65B\n16.7K\u00a0Pulls 73\u00a0Tags Updated\u00a05 weeks ago\ncodeup\nGreat code generation model based on Llama2.\n13B\n15.8K\u00a0Pulls 19\u00a0Tags Updated\u00a06 months ago\nyarn-mistral\nAn extension of Mistral to support context windows of 64K or\n128K.\n7B\n15.3K\u00a0Pulls 33\u00a0Tags Updated\u00a04 months ago\neverythinglm\nUncensored Llama2 based model with support for a 16K\ncontext window.\n13B\n14.7K\u00a0Pulls 18\u00a0Tags Updated\u00a04 months ago\nm",
    "Open-source medical large language model adapted from\nLlama 2 to the medical domain.\n7B 69B\n14.1K\u00a0Pulls 22\u00a0Tags Updated\u00a05 months ago\nllama-pro\nAn expansion of Llama 2 that specializes in integrating both\ngeneral language understanding and domain-specific\nknowledge, particularly in programming and mathematics.\n8B\n14.1K\u00a0Pulls 33\u00a0Tags Updated\u00a04 months ago\nmagicoder\n Magicoder is a family of 7B parameter models trained on\n75K synthetic instruction data using OSS-Instruct, a novel\napproach to enlightening LLMs with open-source code\nsnippets.\n7B\n11.8K\u00a0Pulls 18\u00a0Tags Updated\u00a05 months ag",
    "h to enlightening LLMs with open-source code\nsnippets.\n7B\n11.8K\u00a0Pulls 18\u00a0Tags Updated\u00a05 months ago\nllama3-chatqa\nA model from NVIDIA based on Llama 3 that excels at\nconversational question answering (QA) and retrieval-\naugmented generation (RAG).\n8B 70B\n11.8K\u00a0Pulls 35\u00a0Tags Updated\u00a010 days ago ",
    "nexusraven\nNexus Raven is a 13B instruction tuned model for function\ncalling tasks.\n13B\n11.7K\u00a0Pulls 32\u00a0Tags Updated\u00a04 months ago\nstablelm-zephyr\nA lightweight chat model allowing accurate, and responsive\noutput without requiring high-end hardware.\n3B\n11.7K\u00a0Pulls 17\u00a0Tags Updated\u00a04 months ago\ncodebooga\nA high-performing code instruct model created by merging\ntwo existing code models.\n34B\n11K\u00a0Pulls 16\u00a0Tags Updated\u00a06 months ago\nmistrallite\nMistralLite is a fine-tuned model based on Mistral with\nenhanced capabilities of processing long contexts.\n7B\n10.5K\u00a0Pulls 17\u00a0Tags Updated\u00a06 months a",
    "with\nenhanced capabilities of processing long contexts.\n7B\n10.5K\u00a0Pulls 17\u00a0Tags Updated\u00a06 months ago\nwizard-vicuna\nWizard Vicuna is a 13B parameter model based on Llama 2\ntrained by MelodysDreamj.\n13B\n9,833\u00a0Pulls 17\u00a0Tags Updated\u00a06 months ago ",
    "llava-llama3\nA LLaVA model fine-tuned from Llama 3 Instruct with better\nscores in several benchmarks.\n8B\n8,963\u00a0Pulls 4\u00a0Tags Updated\u00a013 days ago\nsnowflake-arctic-embed\nA suite of text embedding models by Snowflake, optimized for\nperformance.\n23M 33M 109M 137M 334M\n8,524\u00a0Pulls 16\u00a0Tags Updated\u00a04 weeks ago\nmoondream\nmoondream2 is a small vision language model designed to\nrun efficiently on edge devices.\n1B\n8,044\u00a0Pulls 18\u00a0Tags Updated\u00a08 days ago\ngoliath\nA language model created by combining two fine-tuned\nLlama 2 70B models into one.\n118B\n7,916\u00a0Pulls 16\u00a0Tags Updated\u00a06 months ago\nopen-or",
    "two fine-tuned\nLlama 2 70B models into one.\n118B\n7,916\u00a0Pulls 16\u00a0Tags Updated\u00a06 months ago\nopen-orca-platypus2\nMerge of the Open Orca OpenChat model and the Garage-\nbAInd Platypus 2 model. Designed for chat and code\ngeneration.\n13B\n7,854\u00a0Pulls 17\u00a0Tags Updated\u00a06 months ago ",
    "duckdb-nsql\n7B parameter text-to-SQL model made by MotherDuck and\nNumbers Station.\n7B\n7,635\u00a0Pulls 17\u00a0Tags Updated\u00a03 months ago\nnotux\nA top-performing mixture of experts model, fine-tuned with\nhigh-quality data.\n47B\n7,458\u00a0Pulls 18\u00a0Tags Updated\u00a04 months ago\nmegadolphin\nMegaDolphin-2.2-120b is a transformation of Dolphin-2.2-\n70b created by interleaving the model with itself.\n120B\n7,386\u00a0Pulls 19\u00a0Tags Updated\u00a04 months ago\nnotus\nA 7B chat model fine-tuned with high-quality data and based\non Zephyr.\n7B\n6,685\u00a0Pulls 18\u00a0Tags Updated\u00a04 months ago\nalfred\nA robust conversational model designed",
    "Zephyr.\n7B\n6,685\u00a0Pulls 18\u00a0Tags Updated\u00a04 months ago\nalfred\nA robust conversational model designed to be used for both\nchat and instruct use cases.\n42B\n5,345\u00a0Pulls 7\u00a0Tags Updated\u00a06 months ago ",
    "llava-phi3\nA new small LLaVA model fine-tuned from Phi 3 Mini.\n4B\n5,067\u00a0Pulls 4\u00a0Tags Updated\u00a013 days ago\nfalcon2\nFalcon2 is an 11B parameters causal decoder-only model built\nby TII and trained over 5T tokens.\n11B\n3,709\u00a0Pulls 17\u00a0Tags Updated\u00a07 days ago\n\u00a9 2024 Ollama\tBlog Docs GitHub Discord X (Twitter)Meetups ",
    "Ollama\nFounded:\nTeam Size:\nLocation:Palo Alto\n\u00a0\u00a0\u00a0\u00a0\nActive Founders\nJeffrey Morgan\nJeffrey Morgan\nOllama\n\u00a0\u00a0\nMichael Chiang\nhttps:\/\/github.com\/ollama\/ollama\nMichael Chiang\nOllama\n\u00a0\u00a0 ",
    "About\nWhat Happens at YC?ApplyYC Interview GuideFAQPeopleYC Blog\nCompanies\nStartup DirectoryTop CompaniesFounder DirectoryLaunch YC\nStartup Jobs\nAll Jobs\u25e6 Engineering\u25e6 Operations\u25e6 Marketing\u25e6 SalesPioneer Internship Program 2024Startup Job GuideCareer\nCoachingYC Startup Jobs Blog\nFind a Co-Founder\nLibrary\nSAFE\nResources\nStartup SchoolNewsletterFor InvestorsHacker NewsBookface\nOpen main menu\nApply for S2024 batch.Apply\nHome\u203aCompanies\u203aOllama\nOllama\nGet up and running with large language models, locally.\nW21\nActive\nartificial-intelligence\ndeveloper-tools\nopen-source\nPalo Alto\nCompany\nJ",
    " models, locally.\nW21\nActive\nartificial-intelligence\ndeveloper-tools\nopen-source\nPalo Alto\nCompany\nJobs0\nNews\n\u00a0\nhttps:\/\/ollama.com\nGet up and running with large language models, locally.\nLatest News\nOllama is now available as an official Docker image \u00b7 Ollama Blog\nOct 05, 2023\nRun Llama 2 uncensored locally \u00b7 Ollama Blog\nAug 01, 2023 ",
    "Footer\nY Combinator\nPrograms\nYC Program\nStartup School\nWork at a Startup\nCo-Founder Matching\nCompany\nYC Blog\nContact\nPress\nPeople\nCareers\nPrivacy Policy\nNotice at Collection\nSecurity\nTerms of Use\nResources\nStartup Directory\nStartup Library\nInvestors\nSAFE\nHacker News\nLaunch YC\nYC Deals\nMake something people want.\nApply\nTwitterFacebookInstagramLinkedInYoutube\n\u00a9 2024 Y Combinator ",
    "Get up and running with large\nlanguage models.\nRun Llama 3, Phi 3, Mistral, Gemma, and other\nmodels. Customize and create your own.\nDownload \u2193\nAvailable for macOS, Linux,\nand Windows (preview)\nBlogDiscordGitHub\tModelsSign in Download\n\u00a9 2024 Ollama\tBlog Docs GitHub Discord X (Twitter)Meetups ",
    "amples with different open source models with\ndifferent use-cases. This will help you to use any future open\nsource LLM models with ease.\nSo, lets get started with the first example!\nHow to Run the LLama2 Model from Meta\nLlama 2 model is an open-source LLM model from Meta and we'll\ninteract with it like we'd do with ChatGPT (free version), only text\nLearn to code \u2014 free 3,000-hour curriculum\nForum Donate ",
    "APRIL 2, 2024\/#LLMS\nHow to Run Open Source LLMs\nLocally Using Ollama\nSahil Mahapatra\nLearn to code \u2014 free 3,000-hour curriculum\nForum Donate ",
    "This article will guide you through downloading and\nusing Ollama, a powerful tool for interacting with\nopen-source large language models (LLMs) on your\nlocal machine.\nLearn to code \u2014 free 3,000-hour curriculum\nForum Donate ",
    "Unlike closed-source models like ChatGPT, Ollama offers\ntransparency and customization, making it a valuable resource for\ndevelopers and enthusiasts.\nWe'll explore how to download Ollama and interact with two\nexciting open-source LLM models: LLaMA 2, a text-based model\nfrom Meta, and LLaVA, a multimodal model that can handle both\ntext and images.\nHow to Download Ollama\nTo download Ollama, head on to the official website of Ollama and\nhit the download button.\nollama homepage\nLearn to code \u2014 free 3,000-hour curriculum\nForum Donate ",
    "Ollama supports 3 different operating systems, and the Windows\nversion is in preview mode.\nollama download page\nYou can choose the executable file according to your OS and after\nsuccessfully downloading the executable file, you can install it by\nrunning the executable file.\nFor Linux users, you have to execute the command that is being\nshown on the screen instead of downloading an executable file.\nHow to Run Ollama\nTo show you the power of using open source LLMs locally, I'll\npresent multiple examples with different open source models with\ndifferent use-cases. This will help you to use any fut",
    "As a responsible AI language model, I am here to assist you with any quest\nwith:\n1. Answering questions: I can provide information on a wide range of topic\n2. Generating ideas: I can help you brainstorm ideas for creative projects\n3. Writing assistance: I can help you with writing tasks such as proofread\n4. Translation: I can translate text from one language to another.\n5. Summarizing content: I can summarize long pieces of text, such as artic\n6. Creativity: I can help you generate creative ideas for stories, poems, \n7. Language learning: I can assist you in learning a new language by provi\n8.",
    "s for stories, poems, \n7. Language learning: I can assist you in learning a new language by provi\n8. Chatting: I'm here to chat with you and provide a response to any quest\nPlease let me know if there is anything specific you would like me to help\n>>> Send a message (\/? for help)\nSo that is the response that I got from llama2.\nTo exit the program, you can type \/exit.\nLet's now run a multi-modal model where you can send an image\nand ask questions based on that.\nHow to Run the LLaVA Model\nLLaVA is a open-source multi-modal LLM model. A multi-modal\nmodel can take input of multiple types and gener",
    " a open-source multi-modal LLM model. A multi-modal\nmodel can take input of multiple types and generate a response\naccordingly.\nUsing this model, we are now going to pass an image and ask a\nquestion based on that.\nSo, first things first, lets download the model:\nLearn to code \u2014 free 3,000-hour curriculum\nForum Donate ",
    "freeCodeCamp is a donor-supported tax-exempt 501(c)(3) charity organization (United States Federal Tax\nIdentification Number: 82-0779546)\nOur mission: to help people learn to code for free. We accomplish this by creating thousands of videos,\narticles, and interactive coding lessons - all freely available to the public.\nDonations to freeCodeCamp go toward our education initiatives, and help pay for servers, services, and staff.\nYou can make a tax-deductible donation here.\nTrending Guides\nDate Formatting in JS\tJava Iterator Hashmap\tCancel a Merge in Git\nWhat is a Linked List?\tInstall Java in Ubu",
    "matting in JS\tJava Iterator Hashmap\tCancel a Merge in Git\nWhat is a Linked List?\tInstall Java in Ubuntu\tPython Ternary Operator\nFull Stack Career Guide Python Sort Dict by Key Smart Quotes Copy\/Paste\nJavaScript Array Length Sets in Python\tKotlin vs Java\nSQL Temp Table\tHTML Form Basics\tComments in YAML\nPandas Count Rows\tPython End Program\tPython XOR Operator\nPython Dict Has Key\tPython List to String\tExit Function in Python\nString to Array in Java\tPython Import from File Parse a String in Python\nPython Merge Dictionaries Copy a Directory in Linux Reactive Programming Guide\nCenter Text Vertically",
    "ython Merge Dictionaries Copy a Directory in Linux Reactive Programming Guide\nCenter Text Vertically CSS What\u2019s a Greedy Algorithm? Edit Commit Messages in Git\nMobile App\nADVERTISEMENT\nConclusion\nThat's it! With Ollama, you can experiment with powerful LLMs\nlike LLaMA 2 and LLaVA on your own machine.\nDownload Ollama and explore the exciting world of open-source\nlarge language models!\nSahil Mahapatra\nRead more posts.\nIf you read this far, thank the author to show them you care.\nSay Thanks\nLearn to code for free. freeCodeCamp's open source curriculum\nhas helped more than 40,000 people get jobs",
    " to code for free. freeCodeCamp's open source curriculum\nhas helped more than 40,000 people get jobs as developers.\nGet started\nLearn to code \u2014 free 3,000-hour curriculum\nForum Donate ",
    "Our Charity\nAboutAlumni NetworkOpen SourceShopSupportSponsorsAcademic HonestyCode of Conduct\nPrivacy PolicyTerms of ServiceCopyright Policy\nLearn to code \u2014 free 3,000-hour curriculum\nForum Donate "
]